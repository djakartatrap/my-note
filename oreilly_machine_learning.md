# データ確認
## カテゴリデータのチェック方法
人間が入力したデータ等の場合、表記ゆれが起きるので、一旦集計して確認する必要がある。  

### 方法
Pandas の Seriesクラスの value_counts関数で、各行に含まれているユニークな値と頻度を表示させる。

# 訓練データとテストデータを作る
## ダミー変数(ワンホットエンコーディング)を使う場合の注意点
- 出来上がるデータの列項目が、訓練データとテストデータで揃っているか？
  - 操作手順に気をつける
    - 手順A
      1. 全データを一括してダミー化
      1. 訓練データと、テストデータに分ける
  - 上記の手順に沿う事が難しい場合は、sklearn.feature_extraction.DictVectorizerが助けてくれるはず
- カテゴリ変数には見えないデータ(整数値データ)も、実はカテゴリ変数だったって事もあるよ！
  - アンケート仕様上「業種は？ -> 1：農業、2：サービス業・・」みたいなアンケートの回答を、数字で集計したようなデータの場合、見た目は整数データだけど、実際はカテゴリカル。
  - 整数値データを無理やりダミー変数化するのは、pandas.get_dummies関数で、columns=パラメタでダミー化したい列項目名を明示的に指定してやるとよい。
  ```
  demo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)
  display(pd.get_dummies(demo_df, columns=['Integer Feature', 'Categorical Feature']))
  ```

# 特徴量エンジニアリング(feature engineering)
モデルの仕様に合わせたり、モデルの精度を上げるために、特徴量(説明変数)をゴニョゴニョする事。

## 手法一覧
- モデルの仕様に合わせるための手法
  - ダミー変数化
    - カテゴリカルデータが使えない場合に、1,0で表される「ダミー変数」の形式に変換する手法の形式に変換する手法
- モデル精度を上げるための手法
  - ビニング
    - 数値データを、bin に区切って行く
    - 線形モデルで行うと、柔軟性が上がって精度が上がる
    - numpyで簡単につくれる
    ```
    bins = np.linspace(-3, 3, 11)
    which_bin = np.digitize(X, bins=bins)
    ```
    ```
    #ビニングした後は、それをさらにワンホット化する
    #scikitlearnの OneHotEncoderは、整数値で表現されたカテゴリカルデータしか、ワンホットに出来ない。
    from sklearn.preprocessing import OneHotEncoder
    encoder = OneHotEncoder(sparse=False)
    #encoder.fit で、which_bin に現れる整数値のバリエーションを確認させる
    encoder.fit(which_bin)
    #transform で実際に one-hot エンコーディングさせる
    X_binned = encoder.transform(which_bin)
    print(X_binned[:5])
    ```
  - 交互作用特徴量 (interaction feature)
    - 「ビニングしたデータのビン指示子✕実際のデータ」を特徴量としたもの
  - 多項式特徴量 (polynomial feature)
    - ある特徴量のべき乗(^1, ^2, ^3...)した値を、新たな特徴量として追加する手法
    - そうすると、なぜだかまろやかな回帰モデルが出来上がる
    - 線形回帰モデルと組み合わせると、古典的な__多項式回帰モデル__ となる
    - 高次元の多項式回帰モデルは、境界近辺やデータが少ない領域では極端な振る舞いをするのに注意
    - 線形回帰モデル✕多項式特徴量で、かなり柔軟なモデルは出来るが、SVMのような複雑なモデルを用いれば、わざわざ特徴量を新たに準備しなくても似たようなモデルを作れる

## モデルの精度を上げるための勘所
### ほとんどのモデルで言えること
個々の特徴量がガウス分布に従っている時に、最もうまく機能する(精度が上がる)。  
回帰モデルの場合には、目的変数もガウス分布している時に、もっともうまく機能する。

## モデルの精度を下げてしまう要因
- ターゲット(説明変数)と特徴量が、非線形関係
  - 特に回帰モデルでは顕著に精度が下がる

## モデル別で、特徴量エンジニアリングを整理
### 決定木ベースのモデル
自分で重要な相互作用を見つける事ができるので、多くの場合、特徴量を明示的に変換する必要はない。
### 比較的単純なモデル(線形モデル、ナイーブベイズ)
ビニング、多項式、交互作用などの特徴量エンジニアリングの効果が出やすい。
### SVM、最近傍法、ニューラルネットワーク
ビニング、多項式、交互作用が功を奏することもあるが、線形モデルほど大きな影響はない。
