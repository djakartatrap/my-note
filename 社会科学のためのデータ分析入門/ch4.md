#4.1.3 世論調査からの予測
## 予測誤差とは
$\text{予測誤差}= \text{実際の結果} - \text{予測された結果}$

## 平均予測誤差とは
- 各観測データと、予測との差分値の平均値
- 別名「バイアス」とも呼ばれる

## 2乗平均平方根誤差とは
- $ \sqrt{ \frac{1}{n} \Sigma{ (\text{観測値}- \text{予測結果})^2} }$
- 平均予測誤差だと、正の大きな誤差と、負の大きな誤差が打ち消し合っている状況を把握ができないことがあるが、2条平均平方根誤差はそれを感知できる

## 分類における誤分類の種類について
分類問題のうち、特に「勝つか負けるか」「陽性か陰性か」のような、2値分類問題における誤分類の種類を考えてみる。


|実際の結果→|陽性|陰性|
|:-:|:-|:-|
|予測値|   |   |
|陽性|真陽性|偽陽性|
|陰性|偽陰性|真陰性|

上述のような行列のことを、__混同行列(confusion matrix)__ と呼ぶ。

2値分類においては、2種類の誤分類(偽陽性、偽陰性)が存在することになる。

# 4.2 線形回帰
## 4.2.1 顔の見た目と選挙結果
### 仮説：1秒未満での顔の見た目で、選挙結果は予測できる。
#### 予測の仕方
1. 被験者に、実際の選挙の勝者、次点だった人の白黒顔写真を1秒未満見せる
2. その後、両候補者のうち、能力が高い方はどちらかを判断してもらう
3. 最終的に、「能力が高い」と判断された割合が高い候補者を勝者として予測する

試しに、民主党候補者の見た目能力スコアと、実際民主党が勝ったかどうかを表す数値(＝民主党と共和党の得票率の差分値)でプロットしたところ、緩やかな右肩上がりの散布図になった(＝弱い正の相関がある)

## 4.2.2 相関と散布図
### 相関係数で判断できること
2変数間の __線形関係__ を数量化したもの。正の相関係数は1変数が増えればもう片方も増える。負の相関係数は1変数が増えれば、もう片方は減るという、2変数がどれくらい「直線的」な関係にあるかを示す。
__相関係数が低い__ ということは、あくまで __「線形関係」が低い__ ということであって、__「非線形関係」がある場合も多々ある__ 点はしっかり覚えておけな！

## 4.2.3 最小2乗法
相関は2つの変数がどの程度の線形関係にあるかどうかを示すもの。
その「線形関係」は、 __線形モデル(linear model)__ で表せる。

$ Y = \alpha + \beta X + \epsilon $
- $ \alpha :\text{切片, intercept} $
- $ \beta :\text{傾き, slope} $
- $ \epsilon :\text{誤差項, error, 撹乱項(disturbance)} $

### 予測問題に取り組む時の心得
ある現象を予測する際には、あるモデルを選定する事になる。(この章で言えば、線形モデル)
これは、イコール、 __「得られたデータの生成過程(data-generating process)が、そのモデルとほぼほぼ近似しているという仮定を前提としている」__ という事になる。
しかし実際には、森羅万象の現象を100％忠実に再現するモデルがどのようなものなのかは、天才か神にしかわからないので、我々凡人は、上述の仮定をする事すらできない・・・。

ってなっちゃうけど。

統計学の偉い人(George Box)が、「すべてのモデルは間違いであるが、そのうちいくつかは有用である」と言っている。
つまり、そのモデルがそのデータの生成過程を100％正確に表現しているわけではないが、実用的である、という事は多々あるよ、ということらしい(多分)

この事をしっかりアタマにいれておこうね！！！

### 線形モデルの式と、実際のデータから考えだす式を改めて整理する

線形モデルを仮定するので、実際のデータは下記の式で計算できる事になる。

$\hat{Y} = \hat{\alpha} + \hat{\beta}x$
- $\hat{Y}$ : 予測値(predicted value), 当てはめ値(fitted value)
- $\hat{\alpha}$ : 仮定している線形モデルの切片 $\alpha$ の推定値
- $\hat{\beta}$ : 仮定している線形モデルの傾き、 $\beta$ の推定値

ただし、実際のところ予測値が実際の観察値とピッタリ一致する事はない。
観察された結果と予測値の差は、__残差(residual)__ 、または __予測誤差(prediction error)__ と呼ばれ、以下の式で表される。

$ \hat{\epsilon} = Y - \hat{Y}$
- $\hat{\epsilon}$ : 残差。誤差項の推定値という事になるので、hatがついた $\epsilon$ になっている。

### 線形回帰のしくみ
線形モデルに当てはめて、係数の推定値を得る際、一般的には __最小二乗法(least squares)__ という手法が使われる。
#### 最小二乗法をざっくりと
残差( $\hat{\epsilon}$ ) の2乗をすべて足し上げた値を最小にするような係数を解析的に求める手法。
この、残差の2乗の総和の事を、__残差平方和(sum of squared residuals; SSR)__ という。

数式で言えばこうなる。

$ SSR = \displaystyle\sum_{i=1}^n \hat{\epsilon}^2 = \displaystyle\sum_{i=1}^n (Y_i - \hat{Y_i})^2 $

この SSR を最小にすることができる＝誤差が最も少ない状況といえる。

##### ちょっと詳しく見る最小二乗法
内部の解析アルゴリズムはすっ飛ばし、結果だけ書くと、係数の推定値は以下の式で導き出せる事になっている。
$$
\begin{aligned}
\hat\alpha &= \bar{Y} - \beta\bar{X} \\
\hat\beta &= \frac {\displaystyle\sum_{i=1}^{n}(Y_i - \bar{Y})(X_i - \bar{X})} {\displaystyle\sum_{i=1}^{n} (X_i - \bar{X})^2}
\end{aligned}
$$
- $\bar{Y}$ ; 観測値Yの平均
- $\bar{X}$ ; 観測値Xの平均

ここでさらに、モデル作成時に仮定していた下記の式から、
$\hat{Y} = \hat{\alpha} + \hat{\beta}x$
$x$が平均値 $\bar{X}$ の時のyの値を求めると・・・
$$
\begin{aligned}
\hat{\alpha} + \hat{\beta}\bar{X} &= (\bar{Y} - \beta\bar{X}) + \hat{\beta}\bar{X} \\
&= 0
\end{aligned}
$$
となる。
つまり、__最小二乗法の回帰モデルは、観測値のX軸の平均値、Y軸の平均を通る直線になっている__。

更に残差について見てみる。
最小二乗法では、残差の2乗の総和を最小にするように係数の推定値を解析的に導出する。
つまり、残差の平均について見てみると・・・

$$
\begin{aligned}
\hat\epsilon \text{の平均} &= \frac {1} {n} \displaystyle\sum_{i=1}^{n} (Y_i - (\hat\alpha + \hat\beta X_i)) \\
&= \bar{Y} - \hat\alpha - \hat\beta\bar{X} \\
&= 0
\end{aligned}
$$
となる。

ここまでの情報をまとめると・・・
***
__最小二乗法とは__
- 残差平方和を最小にするような切片と傾きを求める手法
- 観測値のX、Y の平均値を通る直線を描く
- 残差の平均はゼロになる
***

### 線形回帰モデルの評価について
 線形回帰の評価をする場合、誤差を RMSE(root mean squraed error) で見る事が多い。それは、この誤差の数式の中に、SSRが入っており、最小二乗法を使う回帰において、評価指標としてはもってこいだからなのだ！

 $ RMSE = \sqrt{\frac{1}{n} SSR} = \sqrt{\frac{1}{n} \displaystyle\sum_{i=1}^{n} \hat\epsilon^2} $

### 相関係数と回帰係数の傾きの不思議な関係

$$
\begin{aligned}
\hat\beta &= \frac {\displaystyle\sum_{i=1}^{n}(Y_i - \bar{Y})(X_i - \bar{X})} {\displaystyle\sum_{i=1}^{n} (X_i - \bar{X})^2} \\
&= \frac {
            \displaystyle\sum_{i=1}^{n}(Y_i - \bar{Y})(X_i - \bar{X})
            \centerdot
            \sqrt{
              \frac{1}{n}
              \displaystyle\sum_{i=1}^{n} (Y_i - \bar{Y})^2
            }
          }  
          { \bigg(
            n \centerdot
            \sqrt{\frac{1}{n} \displaystyle\sum_{i=1}^{n} (X_i - \bar{X})^2 }
            \centerdot
            \sqrt{\frac{1}{n} \displaystyle\sum_{i=1}^{n} (X_i - \bar{X})^2 }
            \bigg)
            \centerdot
            \sqrt{
                \frac{1}{n}
                \displaystyle\sum_{i=1}^{n} (Y_i - \bar{Y})^2
            }
          } \text{   ...分母と分子に}\sqrt{
              \frac{1}{n}
              \displaystyle\sum_{i=1}^{n} (Y_i - \bar{Y})^2
          } \text{をかけた} \\
&= \frac{1}{n} \displaystyle\sum_{i=1}^{n}
  \frac{
    (Y_i - \bar{Y})(X_i - \bar{X})
  }{
    \sqrt{
        \frac{1}{n}
        \displaystyle\sum_{i=1}^{n} (Y_i - \bar{Y})^2
    }
    \sqrt{
      \frac{1}{n}
      \displaystyle\sum_{i=1}^{n} (X_i - \bar{X})^2
    }
  }
  \centerdot
  \frac{
    \sqrt{
      \frac{1}{n}
      \displaystyle\sum_{i=1}^{n} (Y_i - \bar{Y})^2
    }
  }{
    \sqrt{
      \frac{1}{n}
      \displaystyle\sum_{i=1}^{n} (X_i - \bar{X})^2
    }
  } \\
&= \text{XとYの相関係数} \centerdot \frac{\text{Yの標準偏差}}{ \text{Xの標準偏差} } \\
&= \frac{\text{XとYの相関係数} \centerdot \text{Yの標準偏差}}{ \text{Xの標準偏差} }
\end{aligned}
$$

つまり。。。
***
xの値がxの標準偏差1つ分の増加をすると、相関係数×Yの標準偏差分、Yが増えるような直線の傾きになっている。
***

### 平均への回帰
「平均への回帰」と呼ばれる現象がある。それは・・・
***
予測変数の値が分布の平均から大きく離れているサンプルにおいて、結果変数の値が平均側に偏る傾向があるという
経験的現象の事。この傾向自体は、偶然のみによって説明ができる。
***

例えば、線形回帰モデルにおいて、目的変数も結果変数も正規分布していて、相関係数もきれいめに正の相関が出ている場合を考えるとわかりやすい。
説明変数が平均よりも大きく低い側の観察において、線形回帰モデルのハマりが良くなく、どちらかというと結果変数の実際の値が大きめになる事が想定できる。
それは、結果変数自体が正規分布をしているので、そもそも確率的に観察値として低い値が出にくいわけなので、当たり前っちゃ当たり前の事よね。

### モデルの当てはまり
どの程度モデルがデータに当てはまっているか？＝どれくらい正確にモデルが観察を予測しているのか？を判断することは重要ですよね。
そこで、この「モデルの当てはまり度具合」を判断できる指標が __決定係数(coefficient of determination)、$R^2$__ です。
簡単に概念を書くと、観測値の __平均値からの外れ具合__ と、__予測値からの外れ具合__ を比較した時に、予測値からの外れ具合の方が小さければ、モデルとしての当てはまり度合いが良いと言えるんじゃね？という考え方。

実際は、単に外れ具合ではなく、それぞれ平方和で考える。

$$
\begin{aligned}
R^2 &= \frac{
          \displaystyle\sum_{i-1}^{n}(Y_i - \bar{Y})^2 - \displaystyle\sum_{i-1}^{n}(Y_i - \hat{Y_i})^2
        }
        {
          \displaystyle\sum_{i-1}^{n}(Y_i - \bar{Y})^2
        }
\\ &= \frac{
          \text{総平方和(Total Sum of Square)} - \text{残差平方和}
        }
        {
          \text{総平方和}
        }
\end{aligned}
$$

最小二乗法を使う線形回帰では、残差平方和を最小値にするような線形モデルを考えるので、基本は、総平方和 ＞ 残差平方和 という関係になる。
つまり、決定係数は基本、0〜1の間に収まり、1に近いほど(＝残差平方和が0に近いほど)当てはまりが良いモデルとなる。
