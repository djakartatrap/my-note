#4.1.3 世論調査からの予測
## 予測誤差とは
$\text{予測誤差}= \text{実際の結果} - \text{予測された結果}$

## 平均予測誤差とは
- 各観測データと、予測との差分値の平均値
- 別名「バイアス」とも呼ばれる

## 2乗平均平方根誤差とは
- $ \sqrt{ \frac{1}{n} \Sigma{ (\text{観測値}- \text{予測結果})^2} }$
- 平均予測誤差だと、正の大きな誤差と、負の大きな誤差が打ち消し合っている状況を把握ができないことがあるが、2条平均平方根誤差はそれを感知できる

## 分類における誤分類の種類について
分類問題のうち、特に「勝つか負けるか」「陽性か陰性か」のような、2値分類問題における誤分類の種類を考えてみる。


|実際の結果→|陽性|陰性|
|:-:|:-|:-|
|予測値|   |   |
|陽性|真陽性|偽陽性|
|陰性|偽陰性|真陰性|

上述のような行列のことを、__混同行列(confusion matrix)__ と呼ぶ。

2値分類においては、2種類の誤分類(偽陽性、偽陰性)が存在することになる。

# 4.2 線形回帰
## 4.2.1 顔の見た目と選挙結果
### 仮説：1秒未満での顔の見た目で、選挙結果は予測できる。
#### 予測の仕方
1. 被験者に、実際の選挙の勝者、次点だった人の白黒顔写真を1秒未満見せる
2. その後、両候補者のうち、能力が高い方はどちらかを判断してもらう
3. 最終的に、「能力が高い」と判断された割合が高い候補者を勝者として予測する

試しに、民主党候補者の見た目能力スコアと、実際民主党が勝ったかどうかを表す数値(＝民主党と共和党の得票率の差分値)でプロットしたところ、緩やかな右肩上がりの散布図になった(＝弱い正の相関がある)

## 4.2.2 相関と散布図
### 相関係数で判断できること
2変数間の __線形関係__ を数量化したもの。正の相関係数は1変数が増えればもう片方も増える。負の相関係数は1変数が増えれば、もう片方は減るという、2変数がどれくらい「直線的」な関係にあるかを示す。
__相関係数が低い__ ということは、あくまで __「線形関係」が低い__ ということであって、__「非線形関係」がある場合も多々ある__ 点はしっかり覚えておけな！

## 4.2.3 最小2乗法
相関は2つの変数がどの程度の線形関係にあるかどうかを示すもの。
その「線形関係」は、 __線形モデル(linear model)__ で表せる。

$ Y = \alpha + \beta X + \epsilon $
- $ \alpha :\text{切片, intercept} $
- $ \beta :\text{傾き, slope} $
- $ \epsilon :\text{誤差項, error, 撹乱項(disturbance)} $

### 予測問題に取り組む時の心得
ある現象を予測する際には、あるモデルを選定する事になる。(この章で言えば、線形モデル)
これは、イコール、 __「得られたデータの生成過程(data-generating process)が、そのモデルとほぼほぼ近似しているという仮定を前提としている」__ という事になる。
しかし実際には、森羅万象の現象を100％忠実に再現するモデルがどのようなものなのかは、天才か神にしかわからないので、我々凡人は、上述の仮定をする事すらできない・・・。

ってなっちゃうけど。

統計学の偉い人(George Box)が、「すべてのモデルは間違いであるが、そのうちいくつかは有用である」と言っている。
つまり、そのモデルがそのデータの生成過程を100％正確に表現しているわけではないが、実用的である、という事は多々あるよ、ということらしい(多分)

この事をしっかりアタマにいれておこうね！！！

### 線形モデルの式と、実際のデータから考えだす式を改めて整理する

線形モデルを仮定するので、実際のデータは下記の式で計算できる事になる。

$\hat{Y} = \hat{\alpha} + \hat{\beta}x$
- $\hat{Y}$ : 予測値(predicted value), 当てはめ値(fitted value)
- $\hat{\alpha}$ : 仮定している線形モデルの切片 $\alpha$ の推定値
- $\hat{\beta}$ : 仮定している線形モデルの傾き、 $\beta$ の推定値

ただし、実際のところ予測値が実際の観察値とピッタリ一致する事はない。
観察された結果と予測値の差は、__残差(residual)__ 、または __予測誤差(prediction error)__ と呼ばれ、以下の式で表される。

$ \hat{\epsilon} = Y - \hat{Y}$
- $\hat{\epsilon}$ : 残差。誤差項の推定値という事になるので、hatがついた $\epsilon$ になっている。

### 線形回帰のしくみ
線形モデルに当てはめて、係数の推定値を得る際、一般的には __最小二乗法(least squares)__ という手法が使われる。
#### 最小二乗法をざっくりと
残差( $\hat{\epsilon}$ ) の2乗をすべて足し上げた値を最小にするような係数を解析的に求める手法。
この、残差の2乗の総和の事を、__残差平方和(sum of squared residuals; SSR)__ という。

数式で言えばこうなる。

$ SSR = \displaystyle\sum_{i=1}^n \hat{\epsilon}^2 = \displaystyle\sum_{i=1}^n (Y_i - \hat{Y_i})^2 $

この SSR を最小にすることができる＝誤差が最も少ない状況といえる。

##### ちょっと詳しく見る最小二乗法
内部の解析アルゴリズムはすっ飛ばし、結果だけ書くと、係数の推定値は以下の式で導き出せる事になっている。
$$
\begin{aligned}
\hat\alpha &= \bar{Y} - \beta\bar{X} \\
\hat\beta &= \frac {\displaystyle\sum_{i=1}^{n}(Y_i - \bar{Y})(X_i - \bar{X})} {\displaystyle\sum_{i=1}^{n} (X_i - \bar{X})^2}
\end{aligned}
$$
- $\bar{Y}$ ; 観測値Yの平均
- $\bar{X}$ ; 観測値Xの平均

ここでさらに、モデル作成時に仮定していた下記の式から、
$\hat{Y} = \hat{\alpha} + \hat{\beta}x$
$x$が平均値 $\bar{X}$ の時のyの値を求めると・・・
$$
\begin{aligned}
\hat{\alpha} + \hat{\beta}\bar{X} &= (\bar{Y} - \beta\bar{X}) + \hat{\beta}\bar{X} \\
&= 0
\end{aligned}
$$
となる。
つまり、__最小二乗法の回帰モデルは、観測値のX軸の平均値、Y軸の平均を通る直線になっている__。

更に残差について見てみる。
最小二乗法では、残差の2乗の総和を最小にするように係数の推定値を解析的に導出する。
つまり、残差の平均について見てみると・・・

$$
\begin{aligned}
\hat\epsilon \text{の平均} &= \frac {1} {n} \displaystyle\sum_{i=1}^{n} (Y_i - (\hat\alpha + \hat\beta X_i)) \\
&= \bar{Y} - \hat\alpha - \hat\beta\bar{X} \\
&= 0
\end{aligned}
$$
となる。

ここまでの情報をまとめると・・・

__最小二乗法とは__
- 残差平方和を最小にするような切片と傾きを求める手法
- 観測値のX、Y の平均値を通る直線を描く
- 残差の平均はゼロになる

### 線形回帰モデルの評価について
 線形回帰の評価をする場合、誤差を RMSE(root mean squraed error) で見る事が多い。それは、この誤差の数式の中に、SSRが入っており、最小二乗法を使う回帰において、評価指標としてはもってこいだからなのだ！

 $ RMSE = \sqrt{\frac{1}{n} SSR} = \sqrt{\frac{1}{n} \displaystyle\sum_{i=1}^{n} \hat\epsilon^2} $
